{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import spotipy\n",
    "import sys\n",
    "import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id='653cfed9a26e46dca3e3e211ded03c0f',\\\n",
    "                                                      client_secret='a14b7886714c41c383458b26434d60cc')\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "import spotipy\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import scipy.sparse as ssp\n",
    "from collections import Counter, defaultdict\n",
    "import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from __future__ import print_function    \n",
    "import time\n",
    "from scipy import stats as sss\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import pickle\n",
    "from scipy import io\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('validation_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_df = validation_df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate user listen histroy by dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "music = validation_df[['customer_id','track_name']]\n",
    "music.sort_values('customer_id',inplace=True)\n",
    "\n",
    "# track_set = music['track_name'].unique() ######\n",
    "users_, tracks = music.T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(users_,True)\n",
    "user_history = np.split(tracks,index[1:])\n",
    "\n",
    "# index_to_user_dict = dict(enumerate(user,start=0))\n",
    "# index_to_track_id = dict(enumerate(track_set,start=0))\n",
    "\n",
    "# inv_track_id_map = {v: k for k, v in index_to_track_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v={u:defaultdict(int) for u in user}\n",
    "\n",
    "for u,history in zip(user,user_history):\n",
    "        for song in history:\n",
    "            v[u][song]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.load('trainset_Dict.npy').item()\n",
    "index_to_user_dict = np.load('train_user_map.npy').item()\n",
    "index_to_track_id = np.load('train_track_map.npy').item()\n",
    "inv_track_id_map = np.load('inv_track_map.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 303044.97it/s]\n"
     ]
    }
   ],
   "source": [
    "v_user = v.keys()\n",
    "v_f = {}\n",
    "non_ative = []\n",
    "n=0\n",
    "\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    if index_to_user_dict[i] in v_user:\n",
    "        v_f[index_to_user_dict[i]] = v[index_to_user_dict[i]]\n",
    "        \n",
    "    else:\n",
    "        n+=1\n",
    "        non_ative.append(i)\n",
    "        v_f[index_to_user_dict[i]] = t[index_to_user_dict[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3289"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 256237.72it/s]\n"
     ]
    }
   ],
   "source": [
    "track_base = []\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    track_base.extend(list(t[index_to_user_dict[i]].keys()))\n",
    "track_base = set(track_base)\n",
    "\n",
    "col_num = len(track_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 26237.55it/s]\n"
     ]
    }
   ],
   "source": [
    "val_mat = ssp.dok_matrix((15000,col_num))\n",
    "\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    for j in v_f[index_to_user_dict[i]].keys():\n",
    "        if j in track_base:\n",
    "            k = inv_track_id_map[j]\n",
    "            val_mat[i,k] = v_f[index_to_user_dict[i]][index_to_track_id[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.mmwrite(\"val_mat_17.mtx\", val_mat)\n",
    "# val_mat = io.mmread(\"val_mat_17.mtx\")\n",
    "col_num=19977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluate 1: MAE and RMSE of predict with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_cosine = io.mmread(\"user_cosine_17.mtx\")\n",
    "user_item = io.mmread(\"user_item.mtx\")\n",
    "user_cosine = user_cosine.tocsr()\n",
    "user_item = user_item.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_col = np.array(range(col_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_mat = val_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:25<00:00, 589.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of user-based cosine similarity is 0.0381353886374\n",
      "\n",
      "RMSE of user-based cosine similarity is 0.6516424533543341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with user-based cosine similarity\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = user_cosine[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_1 = MAE_numerator/value_denominator\n",
    "RMSE_1 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of user-based cosine similarity is',MAE_1)\n",
    "print('')\n",
    "print('RMSE of user-based cosine similarity is',RMSE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_Pearson = io.mmread(\"user_Pearson_17.mtx\")\n",
    "user_Pearson = user_Pearson.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_Pearson.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:24<00:00, 604.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of user-based Pearson Correlation is 0.00757784929064\n",
      "\n",
      "RMSE of user-based Pearson Correlation is 0.0888499763479117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with user-based Pearson correlation\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = user_Pearson[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_2 = MAE_numerator/value_denominator\n",
    "RMSE_2 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of user-based Pearson Correlation is',MAE_2)\n",
    "print('')\n",
    "print('RMSE of user-based Pearson Correlation is',RMSE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Item_cosine = io.mmread(\"Item_cosine_17.mtx\")\n",
    "Item_cosine = Item_cosine.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:25<00:00, 590.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Item cosine similarity is 0.00611857296165\n",
      "\n",
      "RMSE of Item cosine similarity is 0.0497896665879327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with item-based cosine similarity\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Item_cosine[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_3 = MAE_numerator/value_denominator\n",
    "RMSE_3 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Item cosine similarity is',MAE_3)\n",
    "print('')\n",
    "print('RMSE of Item cosine similarity is',RMSE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Item_adjusted_cosine = io.mmread(\"Item_adjusted_cosine_17.mtx\")\n",
    "Item_adjusted_cosine = Item_adjusted_cosine.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:24<00:00, 601.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Item adjusted cosine similarity is 0.00511745425269\n",
      "\n",
      "RMSE of Item adjusted cosine similarity is 0.053856400199402854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with item-based adjusted cosine similarity\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Item_adjusted_cosine[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_4 = MAE_numerator/value_denominator\n",
    "RMSE_4 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Item adjusted cosine similarity is',MAE_4)\n",
    "print('')\n",
    "print('RMSE of Item adjusted cosine similarity is',RMSE_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Item_Pearson = io.mmread(\"Item_Pearson_17.mtx\")\n",
    "Item_Pearson = Item_Pearson.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:24<00:00, 612.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Item Pearson Correlation is 0.00208821904156\n",
      "\n",
      "RMSE of Item Pearson Correlation is 0.035055190045670835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with item-based Pearson Correlation\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Item_Pearson[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_5 = MAE_numerator/value_denominator\n",
    "RMSE_5 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Item Pearson Correlation is',MAE_5)\n",
    "print('')\n",
    "print('RMSE of Item Pearson Correlation is',RMSE_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [01:10<00:00, 213.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of average of five similarity pred is 0.00926240764814\n",
      "\n",
      "RMSE of average of five similarity pred is 0.1393824411039587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with average of all similarity based results\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    avg_pred = (user_cosine[i,pred_col].toarray()+user_Pearson[i,pred_col].toarray()+\\\n",
    "               Item_cosine[i,pred_col].toarray()+Item_adjusted_cosine[i,pred_col].toarray()+\n",
    "               Item_Pearson[i,pred_col].toarray())/5\n",
    "    minus = avg_pred - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE = MAE_numerator/value_denominator\n",
    "RMSE =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of average of five similarity pred is',MAE)\n",
    "print('')\n",
    "print('RMSE of average of five similarity pred is',RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Content_based = io.mmread(\"Content_based_17.mtx\")\n",
    "Content_based = Content_based.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:32<00:00, 467.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of Content based cosine is 6.42139079337\n",
      "\n",
      "RMSE of Content based cosine is 11.874824639006192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with content-based algorithm\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Content_based[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_6 = MAE_numerator/value_denominator\n",
    "RMSE_6 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Content based cosine is',MAE_6)\n",
    "print('')\n",
    "print('RMSE of Content based cosine is',RMSE_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [01:28<00:00, 173.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE of average of six similarity pred is 1.07755421442\n",
      "\n",
      "RMSE of average of six similarity pred is 1.9884731507232984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Calculate the MAE and RMSE with average of all similarity based results\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    avg_pred = (user_cosine[i,pred_col].toarray()+user_Pearson[i,pred_col].toarray()+\\\n",
    "               Item_cosine[i,pred_col].toarray()+Item_adjusted_cosine[i,pred_col].toarray()+\n",
    "               Item_Pearson[i,pred_col].toarray()+Content_based[i,pred_col].toarray())/6\n",
    "    minus = avg_pred - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE = MAE_numerator/value_denominator\n",
    "RMSE =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of average of six similarity pred is',MAE)\n",
    "print('')\n",
    "print('RMSE of average of six similarity pred is',RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluate 2: Recall and Precision metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"non_active.txt\", \"rb\") as fp:   # Unpickling\n",
    "#     non_ative = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v_f = np.load('v_f.npy').item()\n",
    "# index_to_user_dict = np.load('train_user_map.npy').item()\n",
    "# index_to_track_id = np.load('train_track_map.npy').item()\n",
    "# inv_track_id_map = np.load('inv_track_map.npy').item()\n",
    "# inv_track_id_map = np.load('track_base.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track_base = np.load('track_base.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "active_user = np.array(range(15000))\n",
    "active_user = np.delete(active_user,non_ative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('active_user_17.npy',active_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:00<00:00, 138446.28it/s]\n"
     ]
    }
   ],
   "source": [
    "### filter the new tracks in validation dataset which not shown in training dataset\n",
    "v_ft = {u:defaultdict(int) for u in v_f.keys()}\n",
    "\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    for k in v_f[index_to_user_dict[i]].keys():\n",
    "        if k in track_base:\n",
    "            v_ft[index_to_user_dict[i]][k]=v_f[index_to_user_dict[i]][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('validation_Dict_17',v_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_cosine_U = np.load('recommender_cosine_U.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_based cosine similarity recall is 0.03119329948866448\n",
      "User_based cosine similarity precision 0.02906668943728119\n"
     ]
    }
   ],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_cosine_U[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_cos_u = suc_rec/Tnum_val\n",
    "precision_cos_u = suc_rec/num_p\n",
    "print('User_based cosine similarity recall is',recall_cos_u)\n",
    "print('User_based cosine similarity precision',precision_cos_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_Pearson_U = np.load('recommender_Pearson_U.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_based Pearson correlation recall is 0.013122445613327713\n",
      "User_based Pearson correlation precision 0.012227819998292203\n"
     ]
    }
   ],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_Pearson_U[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_P_u = suc_rec/Tnum_val\n",
    "precision_P_u = suc_rec/num_p\n",
    "print('User_based Pearson correlation recall is',recall_P_u)\n",
    "print('User_based Pearson correlation precision',precision_P_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_cosine_T = np.load('recommender_cosine_T.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track based cosine similarity recall is 0.010794860986382713\n",
      "Track based cosine similarity precision is 0.01005891896507557\n"
     ]
    }
   ],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_cosine_T[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_cos_t = suc_rec/Tnum_val\n",
    "precision_cos_t = suc_rec/num_p\n",
    "print('Track based cosine similarity recall is',recall_cos_t)\n",
    "print('Track based cosine similarity precision is',precision_cos_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_adjusted_cosine_T = np.load('recommender_adjuested_cosine_T.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track based adjusted cosine similarity recall is 0.006322966112567124\n",
      "Track based adjusted cosine similarity precision is 0.005891896507556998\n"
     ]
    }
   ],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_adjusted_cosine_T[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_adcos_t = suc_rec/Tnum_val\n",
    "precision_adcos_t = suc_rec/num_p\n",
    "print('Track based adjusted cosine similarity recall is',recall_adcos_t)\n",
    "print('Track based adjusted cosine similarity precision is',precision_adcos_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_Pearson_T = np.load('recommender_Pearson_T.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track based Pearson correlation recall is 0.009713542143943698\n",
      "Track based Pearson correlation precision is 0.009051319272478866\n"
     ]
    }
   ],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_Pearson_T[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_P_t = suc_rec/Tnum_val\n",
    "precision_P_t = suc_rec/num_p\n",
    "print('Track based Pearson correlation recall is',recall_P_t)\n",
    "print('Track based Pearson correlation precision is',precision_P_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five recommendation sets recall is 0.05190330443707274\n",
      "Five recommendation sets precision is 0.009672957048928357\n"
     ]
    }
   ],
   "source": [
    "### combine all five recommendation sets\n",
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    rec_set = []\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set.extend(recommender_cosine_U[i])\n",
    "    rec_set.extend(recommender_Pearson_U[i])\n",
    "    rec_set.extend(recommender_cosine_T[i])\n",
    "    rec_set.extend(recommender_adjusted_cosine_T[i])\n",
    "    rec_set.extend(recommender_Pearson_T[i])\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall = suc_rec/Tnum_val\n",
    "precision = suc_rec/num_p\n",
    "print('Five recommendation sets recall is',recall)\n",
    "print('Five recommendation sets precision is',precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_conten_based = np.load('recommender_content_based.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content-based recall is 0.00047651338819346443\n",
      "content-based precision is 0.00044402698317820853\n"
     ]
    }
   ],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_conten_based[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_content = suc_rec/Tnum_val\n",
    "precision_content = suc_rec/num_p\n",
    "print('content-based recall is',recall_content)\n",
    "print('content-based precision is',precision_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Six recommendation sets recall is 0.05232483551124388\n",
      "Six recommendation sets precision is 0.008126263057524265\n"
     ]
    }
   ],
   "source": [
    "### combine all six recommendation sets\n",
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    rec_set = []\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set.extend(recommender_cosine_U[i])\n",
    "    rec_set.extend(recommender_Pearson_U[i])\n",
    "    rec_set.extend(recommender_cosine_T[i])\n",
    "    rec_set.extend(recommender_adjusted_cosine_T[i])\n",
    "    rec_set.extend(recommender_Pearson_T[i])\n",
    "    rec_set.extend(recommender_conten_based[i])\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall = suc_rec/Tnum_val\n",
    "precision = suc_rec/num_p\n",
    "print('Six recommendation sets recall is',recall)\n",
    "print('Six recommendation sets precision is',precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #read\n",
    "train = pd.read_csv('train_supervised_17.csv')\n",
    "train = train.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_label = train['ratings']\n",
    "x_input = train.drop('ratings',axis=1)\n",
    "x_user = x_input.drop('customer_id',axis=1)\n",
    "\n",
    "age = pd.DataFrame(2017-train['birth_year'].values)\n",
    "train = pd.concat([train,age],axis=1)\n",
    "train.rename(columns={0:'age'},inplace=True)\n",
    "train.drop('birth_year',axis=1,inplace=True)\n",
    "\n",
    "train.sort_values('customer_id',inplace=True)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(train['customer_id'].values,True)\n",
    "customer = np.split(train['customer_id'].values,index[1:])\n",
    "\n",
    "acout = np.split(train['acousticness'].values,index[1:])\n",
    "rat = np.split(train['ratings'].values,index[1:])\n",
    "danceability = np.split(train['danceability'].values,index[1:])\n",
    "duration = np.split(train['duration_ms'].values,index[1:])\n",
    "energy = np.split(train['energy'].values,index[1:])\n",
    "instrumentalness = np.split(train['instrumentalness'].values,index[1:])\n",
    "key = np.split(train['key'].values,index[1:])\n",
    "liveness = np.split(train['liveness'].values,index[1:])\n",
    "loudness = np.split(train['loudness'].values,index[1:])\n",
    "mode = np.split(train['mode'].values,index[1:])\n",
    "speechiness = np.split(train['speechiness'].values,index[1:])\n",
    "tempo = np.split(train['tempo'].values,index[1:])\n",
    "time_signature = np.split(train['time_signature'].values,index[1:])\n",
    "valence = np.split(train['valence'].values,index[1:])\n",
    "age = np.split(train['age'].values,index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22000/22000 [00:04<00:00, 5320.72it/s]\n"
     ]
    }
   ],
   "source": [
    "cluster_X = []\n",
    "for i in tqdm.tqdm(range(22000)):\n",
    "    rat_ = rat[i]\n",
    "    acout_ = np.sum((acout[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    danceability_ = np.sum((danceability[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    duration_ = np.sum((duration[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    energy_ = np.sum((energy[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    instrumentalness_ = np.sum((instrumentalness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    key_ = np.sum((key[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    liveness_ = np.sum((liveness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    loudness_ = np.sum((loudness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    mode_ = np.sum((mode[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    speechiness_ = np.sum((speechiness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    tempo_ = np.sum((tempo[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    time_signature_ = np.sum((time_signature[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    valence_ = np.sum((valence[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    age_ = np.sum((age[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    cluster_X.append(np.array([acout_,danceability_,duration_,energy_,instrumentalness_,key_,liveness_,loudness_,mode_,\\\n",
    "        speechiness_,tempo_,time_signature_,valence_,age_]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(cluster_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_1 = np.array(np.where(kmeans.labels_==0))[0]\n",
    "group_2 = np.array(np.where(kmeans.labels_==1))[0]\n",
    "group_3 = np.array(np.where(kmeans.labels_==2))[0]\n",
    "group_4 = np.array(np.where(kmeans.labels_==3))[0]\n",
    "group_5 = np.array(np.where(kmeans.labels_==4))[0]\n",
    "group_6 = np.array(np.where(kmeans.labels_==5))[0]\n",
    "# group_7 = np.array(np.where(kmeans.labels_==6))[0]\n",
    "# group_8 = np.array(np.where(kmeans.labels_==7))[0]\n",
    "# group_9 = np.array(np.where(kmeans.labels_==8))[0]\n",
    "# group_10 = np.array(np.where(kmeans.labels_==9))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_recommend = pd.read_csv('test_supervised.csv')\n",
    "raw_recommend = raw_recommend.drop('Unnamed: 0',axis=1)\n",
    "raw_recommend.sort_values('customer_id',inplace=True)\n",
    "raw_recommend.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_recommend.drop_duplicates(subset=['customer_id','track_name'],inplace=True)\n",
    "raw_recommend.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age = pd.DataFrame(2017-raw_recommend['birth_year'].values)\n",
    "raw_recommend = pd.concat([raw_recommend,age],axis=1)\n",
    "raw_recommend.rename(columns={0:'age'},inplace=True)\n",
    "raw_recommend.drop('birth_year',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_recommend['age'] = raw_recommend['age'].fillna(22.0)\n",
    "# raw_recommend['age'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 27.00it/s]\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for j in tqdm.tqdm([group_1,group_2,group_3,group_4,group_5,group_6]):\n",
    "    k+=1\n",
    "    group_list = []\n",
    "    for i in j:\n",
    "        group_list.append(customer[i][0])\n",
    "    if k==1:\n",
    "        test_1 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==2:\n",
    "        test_2 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==3:\n",
    "        test_3 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==4:\n",
    "        test_4 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==5:\n",
    "        test_5 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==6:\n",
    "        test_6 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "#     if k==7:\n",
    "#         group_df_7 = train[train.customer_id.isin(group_list)]\n",
    "#     if k==8:\n",
    "#         group_df_8 = train[train.customer_id.isin(group_list)]\n",
    "#     if k==9:\n",
    "#         group_df_9 = train[train.customer_id.isin(group_list)]\n",
    "#     if k==10:\n",
    "#         group_df_10 = train[train.customer_id.isin(group_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_drop = ['customer_id', 'track_name','artist_name', 'album_name']\n",
    "raw_recommend_train = raw_recommend.drop(features_to_drop,axis=1)\n",
    "test_1 = test_1.drop(features_to_drop,axis=1)\n",
    "test_2 = test_2.drop(features_to_drop,axis=1)\n",
    "test_3 = test_3.drop(features_to_drop,axis=1)\n",
    "test_4 = test_4.drop(features_to_drop,axis=1)\n",
    "test_5 = test_5.drop(features_to_drop,axis=1)\n",
    "test_6 = test_6.drop(features_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_ft=np.load('validation_Dict.npy').item()\n",
    "# active_user = np.load('active_user.npy')\n",
    "# t = np.load('trainset_Dict.npy').item()\n",
    "# index_to_user_dict = np.load('train_user_map.npy').item()\n",
    "# index_to_track_id = np.load('train_track_map.npy').item()\n",
    "# inv_track_id_map = np.load('inv_track_map.npy').item()\n",
    "# track_uni_=np.load('track_feature_1.npy').item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'xgboost.sav'\n",
    "xgboost = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_filter = xgboost.predict(raw_recommend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([raw_recommend[['customer_id','track_name']],pd.DataFrame(y_filter).rename(columns={0:'pre_ratings'})]\\\n",
    "                 ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    293467.000000\n",
       "mean          1.322073\n",
       "std           1.131293\n",
       "min           0.962987\n",
       "25%           1.259450\n",
       "50%           1.281012\n",
       "75%           1.307150\n",
       "max          56.242435\n",
       "Name: pre_ratings, dtype: float64"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred['pre_ratings'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred['customer_id'],True)\n",
    "rec_track = np.split(pred['track_name'],index[1:])\n",
    "pre_track = np.split(pred['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:10<00:00, 1457.83it/s]\n"
     ]
    }
   ],
   "source": [
    "keep_ = 15\n",
    "filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_xgb = suc_rec/Tnum_val\n",
    "precision_xgb = suc_rec/num_p\n",
    "print('xgboost without k-means filtered recall is',recall_xgb)\n",
    "print('xgboost without k-means filtered precision is',precision_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'NeuralNetwork.sav'\n",
    "NN = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_NN = NN.predict(raw_recommend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_NN = pd.concat([raw_recommend[['customer_id','track_name']],pd.DataFrame(y_NN).rename(columns={0:'pre_ratings'})]\\\n",
    "                 ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_NN['customer_id'],True)\n",
    "rec_track = np.split(pred_NN['track_name'],index[1:])\n",
    "pre_track = np.split(pred_NN['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:10<00:00, 1369.98it/s]\n"
     ]
    }
   ],
   "source": [
    "keep_ = 15\n",
    "NN_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    NN_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = NN_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('Neural Network without k-means filtered recall is',recall_nn)\n",
    "print('Neural Network without k-means filtered precision is',precision_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'randaomforest_17.sav'\n",
    "RFR = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_RFR = RFR.predict(raw_recommend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_RFR = pd.concat([raw_recommend[['customer_id','track_name']],pd.DataFrame(y_RFR).rename(columns={0:'pre_ratings'})]\\\n",
    "                 ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_RFR['customer_id'],True)\n",
    "rec_track = np.split(pred_RFR['track_name'],index[1:])\n",
    "pre_track = np.split(pred_RFR['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:10<00:00, 1404.20it/s]\n"
     ]
    }
   ],
   "source": [
    "keep_ = 15\n",
    "RFR_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    RFR_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(RFR_filtered_rec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = RFR_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_rfr = suc_rec/Tnum_val\n",
    "precision_rfr = suc_rec/num_p\n",
    "print('Random Forest without k-means filtered recall is',recall_rfr)\n",
    "print('Random Forest without k-means filtered precision is',precision_rfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network\n",
    "filename = 'nn1.sav'\n",
    "nn1 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn2.sav'\n",
    "nn2 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn3.sav'\n",
    "nn3 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn4.sav'\n",
    "nn4 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn5.sav'\n",
    "nn5 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn6.sav'\n",
    "nn6 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_nn1 = nn1.predict(test_1)\n",
    "y_nn2 = nn2.predict(test_2)\n",
    "y_nn3 = nn3.predict(test_3)\n",
    "y_nn4 = nn4.predict(test_4)\n",
    "y_nn5 = nn5.predict(test_5)\n",
    "y_nn6 = nn6.predict(test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_1 = pd.DataFrame(y_nn1,index=test_1.index).rename(columns={0:'pre_ratings'})\n",
    "pred_2 = pd.DataFrame(y_nn2,index=test_2.index).rename(columns={0:'pre_ratings'})\n",
    "pred_3 = pd.DataFrame(y_nn3,index=test_3.index).rename(columns={0:'pre_ratings'})\n",
    "pred_4 = pd.DataFrame(y_nn4,index=test_4.index).rename(columns={0:'pre_ratings'})\n",
    "pred_5 = pd.DataFrame(y_nn5,index=test_5.index).rename(columns={0:'pre_ratings'})\n",
    "pred_6 = pd.DataFrame(y_nn6,index=test_6.index).rename(columns={0:'pre_ratings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([pred_1,pred_2,pred_3,pred_4,pred_5,pred_6])\n",
    "pred = pred.reset_index().sort_values('index').reset_index()\n",
    "pred.drop(['level_0','index'],axis=1,inplace=True)\n",
    "pred_NN = pd.concat([raw_recommend[['customer_id','track_name']],pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_NN['customer_id'],True)\n",
    "rec_track = np.split(pred_NN['track_name'],index[1:])\n",
    "pre_track = np.split(pred_NN['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:10<00:00, 1395.68it/s]\n"
     ]
    }
   ],
   "source": [
    "keep_ = 15\n",
    "NN_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    NN_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = NN_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('Neural Network with k-means filtered recall is',recall_nn)\n",
    "print('Neural Network with k-means filtered precision is',precision_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgboost\n",
    "filename = 'xgboost_1.sav'\n",
    "xgb_1 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_2.sav'\n",
    "xgb_2 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_3.sav'\n",
    "xgb_3 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_4.sav'\n",
    "xgb_4 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_5.sav'\n",
    "xgb_5 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_6.sav'\n",
    "xgb_6 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_xgb1 = xgb_1.predict(test_1)\n",
    "y_xgb2 = xgb_2.predict(test_2)\n",
    "y_xgb3 = xgb_3.predict(test_3)\n",
    "y_xgb4 = xgb_4.predict(test_4)\n",
    "y_xgb5 = xgb_5.predict(test_5)\n",
    "y_xgb6 = xgb_6.predict(test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_1 = pd.DataFrame(y_xgb1,index=test_1.index).rename(columns={0:'pre_ratings'})\n",
    "pred_2 = pd.DataFrame(y_xgb2,index=test_2.index).rename(columns={0:'pre_ratings'})\n",
    "pred_3 = pd.DataFrame(y_xgb3,index=test_3.index).rename(columns={0:'pre_ratings'})\n",
    "pred_4 = pd.DataFrame(y_xgb4,index=test_4.index).rename(columns={0:'pre_ratings'})\n",
    "pred_5 = pd.DataFrame(y_xgb5,index=test_5.index).rename(columns={0:'pre_ratings'})\n",
    "pred_6 = pd.DataFrame(y_xgb6,index=test_6.index).rename(columns={0:'pre_ratings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([pred_1,pred_2,pred_3,pred_4,pred_5,pred_6])\n",
    "pred = pred.reset_index().sort_values('index').reset_index()\n",
    "pred.drop(['level_0','index'],axis=1,inplace=True)\n",
    "pred_XGB = pd.concat([raw_recommend[['customer_id','track_name']],pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_XGB['customer_id'],True)\n",
    "rec_track = np.split(pred_XGB['track_name'],index[1:])\n",
    "pre_track = np.split(pred_XGB['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:10<00:00, 1424.59it/s]\n"
     ]
    }
   ],
   "source": [
    "keep_ = 15\n",
    "XGB_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    XGB_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = XGB_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('XGB with k-means filtered recall is',recall_nn)\n",
    "print('XGB with k-means filtered precision is',precision_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomforest\n",
    "filename = 'randaomforest_17_1.sav'\n",
    "rf_1 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_2.sav'\n",
    "rf_2 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_3.sav'\n",
    "rf_3 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_4.sav'\n",
    "rf_4 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_5.sav'\n",
    "rf_5 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_6.sav'\n",
    "rf_6 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_rf1 = rf_1.predict(test_1)\n",
    "y_rf2 = rf_2.predict(test_2)\n",
    "y_rf3 = rf_3.predict(test_3)\n",
    "y_rf4 = rf_4.predict(test_4)\n",
    "y_rf5 = rf_5.predict(test_5)\n",
    "y_rf6 = rf_6.predict(test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_1 = pd.DataFrame(y_rf1,index=test_1.index).rename(columns={0:'pre_ratings'})\n",
    "pred_2 = pd.DataFrame(y_rf2,index=test_2.index).rename(columns={0:'pre_ratings'})\n",
    "pred_3 = pd.DataFrame(y_rf3,index=test_3.index).rename(columns={0:'pre_ratings'})\n",
    "pred_4 = pd.DataFrame(y_rf4,index=test_4.index).rename(columns={0:'pre_ratings'})\n",
    "pred_5 = pd.DataFrame(y_rf5,index=test_5.index).rename(columns={0:'pre_ratings'})\n",
    "pred_6 = pd.DataFrame(y_rf6,index=test_6.index).rename(columns={0:'pre_ratings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([pred_1,pred_2,pred_3,pred_4,pred_5,pred_6])\n",
    "pred = pred.reset_index().sort_values('index').reset_index()\n",
    "pred.drop(['level_0','index'],axis=1,inplace=True)\n",
    "pred_RF = pd.concat([raw_recommend[['customer_id','track_name']],pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_RF['customer_id'],True)\n",
    "rec_track = np.split(pred_RF['track_name'],index[1:])\n",
    "pre_track = np.split(pred_RF['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:11<00:00, 1354.07it/s]\n"
     ]
    }
   ],
   "source": [
    "keep_ = 15\n",
    "RF_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    RF_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = RF_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('RF with k-means filtered recall is',recall_nn)\n",
    "print('RF with k-means filtered precision is',precision_nn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
