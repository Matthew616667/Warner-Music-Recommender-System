{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "import spotipy\n",
    "import sys\n",
    "import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id='653cfed9a26e46dca3e3e211ded03c0f',\\\n",
    "                                                      client_secret='a14b7886714c41c383458b26434d60cc')\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n",
    "import spotipy\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "import scipy.sparse as ssp\n",
    "from collections import Counter, defaultdict\n",
    "import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.stats import pearsonr\n",
    "from __future__ import print_function    \n",
    "import time\n",
    "from scipy import stats as sss\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import pickle\n",
    "from scipy import io\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_df = pd.read_csv('validation_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_df = validation_df.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate user listen histroy by dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "music = validation_df[['customer_id','track_name']]\n",
    "music.sort_values('customer_id',inplace=True)\n",
    "\n",
    "# track_set = music['track_name'].unique() ######\n",
    "users_, tracks = music.T.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(users_,True)\n",
    "user_history = np.split(tracks,index[1:])\n",
    "\n",
    "# index_to_user_dict = dict(enumerate(user,start=0))\n",
    "# index_to_track_id = dict(enumerate(track_set,start=0))\n",
    "\n",
    "# inv_track_id_map = {v: k for k, v in index_to_track_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v={u:defaultdict(int) for u in user}\n",
    "\n",
    "for u,history in zip(user,user_history):\n",
    "        for song in history:\n",
    "            v[u][song]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = np.load('trainset_Dict.npy').item()\n",
    "index_to_user_dict = np.load('train_user_map.npy').item()\n",
    "index_to_track_id = np.load('train_track_map.npy').item()\n",
    "inv_track_id_map = np.load('inv_track_map.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_user = v.keys()\n",
    "v_f = {}\n",
    "non_ative = []\n",
    "n=0\n",
    "\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    if index_to_user_dict[i] in v_user:\n",
    "        v_f[index_to_user_dict[i]] = v[index_to_user_dict[i]]\n",
    "        \n",
    "    else:\n",
    "        n+=1\n",
    "        non_ative.append(i)\n",
    "        v_f[index_to_user_dict[i]] = t[index_to_user_dict[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "track_base = []\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    track_base.extend(list(t[index_to_user_dict[i]].keys()))\n",
    "track_base = set(track_base)\n",
    "\n",
    "col_num = len(track_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_mat = ssp.dok_matrix((15000,col_num))\n",
    "\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    for j in v_f[index_to_user_dict[i]].keys():\n",
    "        if j in track_base:\n",
    "            k = inv_track_id_map[j]\n",
    "            val_mat[i,k] = v_f[index_to_user_dict[i]][index_to_track_id[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "io.mmwrite(\"val_mat_17.mtx\", val_mat)\n",
    "# val_mat = io.mmread(\"val_mat_17.mtx\")\n",
    "col_num=19977"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluate 1: MAE and RMSE of predict with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_cosine = io.mmread(\"user_cosine_17.mtx\")\n",
    "user_item = io.mmread(\"user_item.mtx\")\n",
    "user_cosine = user_cosine.tocsr()\n",
    "user_item = user_item.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat_col = np.array(range(col_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_mat = val_mat.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with user-based cosine similarity\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = user_cosine[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_1 = MAE_numerator/value_denominator\n",
    "RMSE_1 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of user-based cosine similarity is',MAE_1)\n",
    "print('')\n",
    "print('RMSE of user-based cosine similarity is',RMSE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_Pearson = io.mmread(\"user_Pearson_17.mtx\")\n",
    "user_Pearson = user_Pearson.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user_Pearson.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with user-based Pearson correlation\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = user_Pearson[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_2 = MAE_numerator/value_denominator\n",
    "RMSE_2 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of user-based Pearson Correlation is',MAE_2)\n",
    "print('')\n",
    "print('RMSE of user-based Pearson Correlation is',RMSE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Item_cosine = io.mmread(\"Item_cosine_17.mtx\")\n",
    "Item_cosine = Item_cosine.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with item-based cosine similarity\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Item_cosine[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_3 = MAE_numerator/value_denominator\n",
    "RMSE_3 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Item cosine similarity is',MAE_3)\n",
    "print('')\n",
    "print('RMSE of Item cosine similarity is',RMSE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Item_adjusted_cosine = io.mmread(\"Item_adjusted_cosine_17.mtx\")\n",
    "Item_adjusted_cosine = Item_adjusted_cosine.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with item-based adjusted cosine similarity\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Item_adjusted_cosine[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_4 = MAE_numerator/value_denominator\n",
    "RMSE_4 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Item adjusted cosine similarity is',MAE_4)\n",
    "print('')\n",
    "print('RMSE of Item adjusted cosine similarity is',RMSE_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Item_Pearson = io.mmread(\"Item_Pearson_17.mtx\")\n",
    "Item_Pearson = Item_Pearson.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with item-based Pearson Correlation\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Item_Pearson[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_5 = MAE_numerator/value_denominator\n",
    "RMSE_5 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Item Pearson Correlation is',MAE_5)\n",
    "print('')\n",
    "print('RMSE of Item Pearson Correlation is',RMSE_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with average of all similarity based results\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    avg_pred = (user_cosine[i,pred_col].toarray()+user_Pearson[i,pred_col].toarray()+\\\n",
    "               Item_cosine[i,pred_col].toarray()+Item_adjusted_cosine[i,pred_col].toarray()+\n",
    "               Item_Pearson[i,pred_col].toarray())/5\n",
    "    minus = avg_pred - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE = MAE_numerator/value_denominator\n",
    "RMSE =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of average of five similarity pred is',MAE)\n",
    "print('')\n",
    "print('RMSE of average of five similarity pred is',RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Content_based = io.mmread(\"Content_based_17.mtx\")\n",
    "Content_based = Content_based.tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with content-based algorithm\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    minus = Content_based[i,pred_col].toarray() - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE_6 = MAE_numerator/value_denominator\n",
    "RMSE_6 =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of Content based cosine is',MAE_6)\n",
    "print('')\n",
    "print('RMSE of Content based cosine is',RMSE_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Calculate the MAE and RMSE with average of all similarity based results\n",
    "value_denominator = 0\n",
    "MAE_numerator = 0\n",
    "RMSE_numerator = 0\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    del_col = user_item[i,:].indices\n",
    "    pred_col = np.delete(mat_col,del_col)\n",
    "    avg_pred = (user_cosine[i,pred_col].toarray()+user_Pearson[i,pred_col].toarray()+\\\n",
    "               Item_cosine[i,pred_col].toarray()+Item_adjusted_cosine[i,pred_col].toarray()+\n",
    "               Item_Pearson[i,pred_col].toarray()+Content_based[i,pred_col].toarray())/6\n",
    "    minus = avg_pred - val_mat[i,pred_col].toarray()\n",
    "    one_MAE = np.sum(np.absolute(minus))\n",
    "    one_RMSE = np.sum(minus**2)\n",
    "    MAE_numerator = MAE_numerator + one_MAE\n",
    "    RMSE_numerator = RMSE_numerator + one_RMSE              \n",
    "    value_denominator = value_denominator + len(pred_col)\n",
    "    \n",
    "MAE = MAE_numerator/value_denominator\n",
    "RMSE =  math.sqrt(RMSE_numerator/value_denominator)    \n",
    "\n",
    "print('MAE of average of six similarity pred is',MAE)\n",
    "print('')\n",
    "print('RMSE of average of six similarity pred is',RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluate 2: Recall and Precision metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open(\"non_active.txt\", \"rb\") as fp:   # Unpickling\n",
    "#     non_ative = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# v_f = np.load('v_f.npy').item()\n",
    "# index_to_user_dict = np.load('train_user_map.npy').item()\n",
    "# index_to_track_id = np.load('train_track_map.npy').item()\n",
    "# inv_track_id_map = np.load('inv_track_map.npy').item()\n",
    "# inv_track_id_map = np.load('track_base.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# track_base = np.load('track_base.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "active_user = np.array(range(15000))\n",
    "active_user = np.delete(active_user,non_ative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('active_user_17.npy',active_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### filter the new tracks in validation dataset which not shown in training dataset\n",
    "v_ft = {u:defaultdict(int) for u in v_f.keys()}\n",
    "\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    for k in v_f[index_to_user_dict[i]].keys():\n",
    "        if k in track_base:\n",
    "            v_ft[index_to_user_dict[i]][k]=v_f[index_to_user_dict[i]][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('validation_Dict_17',v_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_cosine_U = np.load('recommender_cosine_U_10.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_cosine_U[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_cos_u = suc_rec/Tnum_val\n",
    "precision_cos_u = suc_rec/num_p\n",
    "print('User_based cosine similarity recall is',recall_cos_u)\n",
    "print('User_based cosine similarity precision',precision_cos_u)\n",
    "f1 = 2*(recall_cos_u*precision_cos_u)/(precision_cos_u+recall_cos_u)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_Pearson_U = np.load('recommender_Pearson_U_10.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_Pearson_U[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_P_u = suc_rec/Tnum_val\n",
    "precision_P_u = suc_rec/num_p\n",
    "print('User_based Pearson correlation recall is',recall_P_u)\n",
    "print('User_based Pearson correlation precision',precision_P_u)\n",
    "f1 = 2*(recall_P_u*precision_P_u)/(precision_P_u+recall_P_u)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_cosine_T = np.load('recommender_cosine_T_10.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_cosine_T[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_cos_t = suc_rec/Tnum_val\n",
    "precision_cos_t = suc_rec/num_p\n",
    "print('Track based cosine similarity recall is',recall_cos_t)\n",
    "print('Track based cosine similarity precision is',precision_cos_t)\n",
    "f1 = 2*(recall_cos_t*precision_cos_t)/(precision_cos_t+recall_cos_t)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_adjusted_cosine_T = np.load('recommender_adjuested_cosine_T_10.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_adjusted_cosine_T[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_adcos_t = suc_rec/Tnum_val\n",
    "precision_adcos_t = suc_rec/num_p\n",
    "print('Track based adjusted cosine similarity recall is',recall_adcos_t)\n",
    "print('Track based adjusted cosine similarity precision is',precision_adcos_t)\n",
    "f1 = 2*(recall_adcos_t*precision_adcos_t)/(precision_adcos_t+recall_adcos_t)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_Pearson_T = np.load('recommender_Pearson_T_10.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_Pearson_T[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_P_t = suc_rec/Tnum_val\n",
    "precision_P_t = suc_rec/num_p\n",
    "print('Track based Pearson correlation recall is',recall_P_t)\n",
    "print('Track based Pearson correlation precision is',precision_P_t)\n",
    "f1 = 2*(recall_P_t*precision_P_t)/(precision_P_t+recall_P_t)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### combine all five recommendation sets\n",
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    rec_set = []\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set.extend(recommender_cosine_U[i])\n",
    "    rec_set.extend(recommender_Pearson_U[i])\n",
    "    rec_set.extend(recommender_cosine_T[i])\n",
    "    rec_set.extend(recommender_adjusted_cosine_T[i])\n",
    "    rec_set.extend(recommender_Pearson_T[i])\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall = suc_rec/Tnum_val\n",
    "precision = suc_rec/num_p\n",
    "print('Five recommendation sets recall is',recall)\n",
    "print('Five recommendation sets precision is',precision)\n",
    "f1 = 2*(recall*precision)/(precision+recall)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recommender_conten_based = np.load('recommender_content_based_10.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = recommender_conten_based[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_content = suc_rec/Tnum_val\n",
    "precision_content = suc_rec/num_p\n",
    "print('content-based recall is',recall_content)\n",
    "print('content-based precision is',precision_content)\n",
    "f1 = 2*(recall_content*precision_content)/(precision_content+recall_content)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### combine all six recommendation sets\n",
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    rec_set = []\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set.extend(recommender_cosine_U[i])\n",
    "    rec_set.extend(recommender_Pearson_U[i])\n",
    "    rec_set.extend(recommender_cosine_T[i])\n",
    "    rec_set.extend(recommender_adjusted_cosine_T[i])\n",
    "    rec_set.extend(recommender_Pearson_T[i])\n",
    "    rec_set.extend(recommender_conten_based[i])\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall = suc_rec/Tnum_val\n",
    "precision = suc_rec/num_p\n",
    "print('Six recommendation sets recall is',recall)\n",
    "print('Six recommendation sets precision is',precision)\n",
    "f1 = 2*(recall*precision)/(precision+recall)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #read\n",
    "train = pd.read_csv('train_supervised_17.csv')\n",
    "train = train.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "y_label = train['ratings']\n",
    "x_input = train.drop('ratings',axis=1)\n",
    "x_user = x_input.drop('customer_id',axis=1)\n",
    "\n",
    "age = pd.DataFrame(2017-train['birth_year'].values)\n",
    "train = pd.concat([train,age],axis=1)\n",
    "train.rename(columns={0:'age'},inplace=True)\n",
    "train.drop('birth_year',axis=1,inplace=True)\n",
    "\n",
    "train.sort_values('customer_id',inplace=True)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(train['customer_id'].values,True)\n",
    "customer = np.split(train['customer_id'].values,index[1:])\n",
    "\n",
    "acout = np.split(train['acousticness'].values,index[1:])\n",
    "rat = np.split(train['ratings'].values,index[1:])\n",
    "danceability = np.split(train['danceability'].values,index[1:])\n",
    "duration = np.split(train['duration_ms'].values,index[1:])\n",
    "energy = np.split(train['energy'].values,index[1:])\n",
    "instrumentalness = np.split(train['instrumentalness'].values,index[1:])\n",
    "key = np.split(train['key'].values,index[1:])\n",
    "liveness = np.split(train['liveness'].values,index[1:])\n",
    "loudness = np.split(train['loudness'].values,index[1:])\n",
    "mode = np.split(train['mode'].values,index[1:])\n",
    "speechiness = np.split(train['speechiness'].values,index[1:])\n",
    "tempo = np.split(train['tempo'].values,index[1:])\n",
    "time_signature = np.split(train['time_signature'].values,index[1:])\n",
    "valence = np.split(train['valence'].values,index[1:])\n",
    "age = np.split(train['age'].values,index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_X = []\n",
    "for i in tqdm.tqdm(range(22000)):\n",
    "    rat_ = rat[i]\n",
    "    acout_ = np.sum((acout[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    danceability_ = np.sum((danceability[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    duration_ = np.sum((duration[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    energy_ = np.sum((energy[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    instrumentalness_ = np.sum((instrumentalness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    key_ = np.sum((key[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    liveness_ = np.sum((liveness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    loudness_ = np.sum((loudness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    mode_ = np.sum((mode[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    speechiness_ = np.sum((speechiness[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    tempo_ = np.sum((tempo[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    time_signature_ = np.sum((time_signature[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    valence_ = np.sum((valence[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    age_ = np.sum((age[i]*rat[i]).astype(float)/np.sum(rat[i]))\n",
    "    cluster_X.append(np.array([acout_,danceability_,duration_,energy_,instrumentalness_,key_,liveness_,loudness_,mode_,\\\n",
    "        speechiness_,tempo_,time_signature_,valence_,age_]))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(cluster_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_1 = np.array(np.where(kmeans.labels_==0))[0]\n",
    "group_2 = np.array(np.where(kmeans.labels_==1))[0]\n",
    "group_3 = np.array(np.where(kmeans.labels_==2))[0]\n",
    "group_4 = np.array(np.where(kmeans.labels_==3))[0]\n",
    "group_5 = np.array(np.where(kmeans.labels_==4))[0]\n",
    "group_6 = np.array(np.where(kmeans.labels_==5))[0]\n",
    "# group_7 = np.array(np.where(kmeans.labels_==6))[0]\n",
    "# group_8 = np.array(np.where(kmeans.labels_==7))[0]\n",
    "# group_9 = np.array(np.where(kmeans.labels_==8))[0]\n",
    "# group_10 = np.array(np.where(kmeans.labels_==9))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_recommend = pd.read_csv('test_supervised_10.csv')\n",
    "raw_recommend = raw_recommend.drop('Unnamed: 0',axis=1)\n",
    "raw_recommend.sort_values('customer_id',inplace=True)\n",
    "raw_recommend.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_recommend.drop_duplicates(subset=['customer_id','track_name'],inplace=True)\n",
    "raw_recommend.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "age = pd.DataFrame(2017-raw_recommend['birth_year'].values)\n",
    "raw_recommend = pd.concat([raw_recommend,age],axis=1)\n",
    "raw_recommend.rename(columns={0:'age'},inplace=True)\n",
    "raw_recommend.drop('birth_year',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_recommend['age'] = raw_recommend['age'].fillna(22.0)\n",
    "# raw_recommend['age'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=0\n",
    "for j in tqdm.tqdm([group_1,group_2,group_3,group_4,group_5,group_6]):\n",
    "    k+=1\n",
    "    group_list = []\n",
    "    for i in j:\n",
    "        group_list.append(customer[i][0])\n",
    "    if k==1:\n",
    "        test_1 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==2:\n",
    "        test_2 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==3:\n",
    "        test_3 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==4:\n",
    "        test_4 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==5:\n",
    "        test_5 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "    if k==6:\n",
    "        test_6 = raw_recommend[raw_recommend.customer_id.isin(group_list)]\n",
    "#     if k==7:\n",
    "#         group_df_7 = train[train.customer_id.isin(group_list)]\n",
    "#     if k==8:\n",
    "#         group_df_8 = train[train.customer_id.isin(group_list)]\n",
    "#     if k==9:\n",
    "#         group_df_9 = train[train.customer_id.isin(group_list)]\n",
    "#     if k==10:\n",
    "#         group_df_10 = train[train.customer_id.isin(group_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_drop = ['customer_id', 'track_name','artist_name', 'album_name']\n",
    "raw_recommend_train = raw_recommend.drop(features_to_drop,axis=1)\n",
    "test_1 = test_1.drop(features_to_drop,axis=1)\n",
    "test_2 = test_2.drop(features_to_drop,axis=1)\n",
    "test_3 = test_3.drop(features_to_drop,axis=1)\n",
    "test_4 = test_4.drop(features_to_drop,axis=1)\n",
    "test_5 = test_5.drop(features_to_drop,axis=1)\n",
    "test_6 = test_6.drop(features_to_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v_ft=np.load('validation_Dict.npy').item()\n",
    "# active_user = np.load('active_user.npy')\n",
    "# t = np.load('trainset_Dict.npy').item()\n",
    "# index_to_user_dict = np.load('train_user_map.npy').item()\n",
    "# index_to_track_id = np.load('train_track_map.npy').item()\n",
    "# inv_track_id_map = np.load('inv_track_map.npy').item()\n",
    "# track_uni_=np.load('track_feature_1.npy').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'xgboost.sav'\n",
    "xgboost = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_filter = xgboost.predict(raw_recommend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([raw_recommend[['customer_id','track_name']],pd.DataFrame(y_filter).rename(columns={0:'pre_ratings'})]\\\n",
    "                 ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred['pre_ratings'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred['customer_id'],True)\n",
    "rec_track = np.split(pred['track_name'],index[1:])\n",
    "pre_track = np.split(pred['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ = 15\n",
    "filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_xgb = suc_rec/Tnum_val\n",
    "precision_xgb = suc_rec/num_p\n",
    "print('xgboost without k-means filtered recall is',recall_xgb)\n",
    "print('xgboost without k-means filtered precision is',precision_xgb)\n",
    "f1 = 2*(recall_xgb*precision_xgb)/(precision_xgb+recall_xgb)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'NeuralNetwork.sav'\n",
    "NN = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_NN = NN.predict(raw_recommend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_NN = pd.concat([raw_recommend[['customer_id','track_name']],pd.DataFrame(y_NN).rename(columns={0:'pre_ratings'})]\\\n",
    "                 ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_NN['customer_id'],True)\n",
    "rec_track = np.split(pred_NN['track_name'],index[1:])\n",
    "pre_track = np.split(pred_NN['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ = 15\n",
    "NN_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    NN_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = NN_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('Neural Network without k-means filtered recall is',recall_nn)\n",
    "print('Neural Network without k-means filtered precision is',precision_nn)\n",
    "f1 = 2*(recall_nn*precision_nn)/(precision_nn+recall_nn)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'randaomforest_17.sav'\n",
    "RFR = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_RFR = RFR.predict(raw_recommend_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_RFR = pd.concat([raw_recommend[['customer_id','track_name']],pd.DataFrame(y_RFR).rename(columns={0:'pre_ratings'})]\\\n",
    "                 ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_RFR['customer_id'],True)\n",
    "rec_track = np.split(pred_RFR['track_name'],index[1:])\n",
    "pre_track = np.split(pred_RFR['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ = 15\n",
    "RFR_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    RFR_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(RFR_filtered_rec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = RFR_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_rfr = suc_rec/Tnum_val\n",
    "precision_rfr = suc_rec/num_p\n",
    "print('Random Forest without k-means filtered recall is',recall_rfr)\n",
    "print('Random Forest without k-means filtered precision is',precision_rfr)\n",
    "f1 = 2*(recall_rfr*precision_rfr)/(precision_rfr+recall_rfr)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## with K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neural network\n",
    "filename = 'nn1.sav'\n",
    "nn1 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn2.sav'\n",
    "nn2 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn3.sav'\n",
    "nn3 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn4.sav'\n",
    "nn4 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn5.sav'\n",
    "nn5 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'nn6.sav'\n",
    "nn6 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_nn1 = nn1.predict(test_1)\n",
    "y_nn2 = nn2.predict(test_2)\n",
    "y_nn3 = nn3.predict(test_3)\n",
    "y_nn4 = nn4.predict(test_4)\n",
    "y_nn5 = nn5.predict(test_5)\n",
    "y_nn6 = nn6.predict(test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_1 = pd.DataFrame(y_nn1,index=test_1.index).rename(columns={0:'pre_ratings'})\n",
    "pred_2 = pd.DataFrame(y_nn2,index=test_2.index).rename(columns={0:'pre_ratings'})\n",
    "pred_3 = pd.DataFrame(y_nn3,index=test_3.index).rename(columns={0:'pre_ratings'})\n",
    "pred_4 = pd.DataFrame(y_nn4,index=test_4.index).rename(columns={0:'pre_ratings'})\n",
    "pred_5 = pd.DataFrame(y_nn5,index=test_5.index).rename(columns={0:'pre_ratings'})\n",
    "pred_6 = pd.DataFrame(y_nn6,index=test_6.index).rename(columns={0:'pre_ratings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([pred_1,pred_2,pred_3,pred_4,pred_5,pred_6])\n",
    "pred = pred.reset_index().sort_values('index').reset_index()\n",
    "pred.drop(['level_0','index'],axis=1,inplace=True)\n",
    "pred_NN = pd.concat([raw_recommend[['customer_id','track_name']],pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_NN['customer_id'],True)\n",
    "rec_track = np.split(pred_NN['track_name'],index[1:])\n",
    "pre_track = np.split(pred_NN['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ = 15\n",
    "NN_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    NN_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = NN_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('Neural Network with k-means filtered recall is',recall_nn)\n",
    "print('Neural Network with k-means filtered precision is',precision_nn)\n",
    "f1 = 2*(recall_nn*precision_nn)/(precision_nn+recall_nn)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# xgboost\n",
    "filename = 'xgboost_1.sav'\n",
    "xgb_1 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_2.sav'\n",
    "xgb_2 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_3.sav'\n",
    "xgb_3 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_4.sav'\n",
    "xgb_4 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_5.sav'\n",
    "xgb_5 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'xgboost_6.sav'\n",
    "xgb_6 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_xgb1 = xgb_1.predict(test_1)\n",
    "y_xgb2 = xgb_2.predict(test_2)\n",
    "y_xgb3 = xgb_3.predict(test_3)\n",
    "y_xgb4 = xgb_4.predict(test_4)\n",
    "y_xgb5 = xgb_5.predict(test_5)\n",
    "y_xgb6 = xgb_6.predict(test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_1 = pd.DataFrame(y_xgb1,index=test_1.index).rename(columns={0:'pre_ratings'})\n",
    "pred_2 = pd.DataFrame(y_xgb2,index=test_2.index).rename(columns={0:'pre_ratings'})\n",
    "pred_3 = pd.DataFrame(y_xgb3,index=test_3.index).rename(columns={0:'pre_ratings'})\n",
    "pred_4 = pd.DataFrame(y_xgb4,index=test_4.index).rename(columns={0:'pre_ratings'})\n",
    "pred_5 = pd.DataFrame(y_xgb5,index=test_5.index).rename(columns={0:'pre_ratings'})\n",
    "pred_6 = pd.DataFrame(y_xgb6,index=test_6.index).rename(columns={0:'pre_ratings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([pred_1,pred_2,pred_3,pred_4,pred_5,pred_6])\n",
    "pred = pred.reset_index().sort_values('index').reset_index()\n",
    "pred.drop(['level_0','index'],axis=1,inplace=True)\n",
    "pred_XGB = pd.concat([raw_recommend[['customer_id','track_name']],pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_XGB['customer_id'],True)\n",
    "rec_track = np.split(pred_XGB['track_name'],index[1:])\n",
    "pre_track = np.split(pred_XGB['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ = 15\n",
    "XGB_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    XGB_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = XGB_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('XGB with k-means filtered recall is',recall_nn)\n",
    "print('XGB with k-means filtered precision is',precision_nn)\n",
    "f1 = 2*(recall_nn*precision_nn)/(precision_nn+recall_nn)\n",
    "print('F1 is',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomforest\n",
    "filename = 'randaomforest_17_1.sav'\n",
    "rf_1 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_2.sav'\n",
    "rf_2 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_3.sav'\n",
    "rf_3 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_4.sav'\n",
    "rf_4 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_5.sav'\n",
    "rf_5 = pickle.load(open(filename, 'rb'))\n",
    "filename = 'randaomforest_17_6.sav'\n",
    "rf_6 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_rf1 = rf_1.predict(test_1)\n",
    "y_rf2 = rf_2.predict(test_2)\n",
    "y_rf3 = rf_3.predict(test_3)\n",
    "y_rf4 = rf_4.predict(test_4)\n",
    "y_rf5 = rf_5.predict(test_5)\n",
    "y_rf6 = rf_6.predict(test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_1 = pd.DataFrame(y_rf1,index=test_1.index).rename(columns={0:'pre_ratings'})\n",
    "pred_2 = pd.DataFrame(y_rf2,index=test_2.index).rename(columns={0:'pre_ratings'})\n",
    "pred_3 = pd.DataFrame(y_rf3,index=test_3.index).rename(columns={0:'pre_ratings'})\n",
    "pred_4 = pd.DataFrame(y_rf4,index=test_4.index).rename(columns={0:'pre_ratings'})\n",
    "pred_5 = pd.DataFrame(y_rf5,index=test_5.index).rename(columns={0:'pre_ratings'})\n",
    "pred_6 = pd.DataFrame(y_rf6,index=test_6.index).rename(columns={0:'pre_ratings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = pd.concat([pred_1,pred_2,pred_3,pred_4,pred_5,pred_6])\n",
    "pred = pred.reset_index().sort_values('index').reset_index()\n",
    "pred.drop(['level_0','index'],axis=1,inplace=True)\n",
    "pred_RF = pd.concat([raw_recommend[['customer_id','track_name']],pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user, index = np.unique(pred_RF['customer_id'],True)\n",
    "rec_track = np.split(pred_RF['track_name'],index[1:])\n",
    "pre_track = np.split(pred_RF['pre_ratings'],index[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_ = 15\n",
    "RF_filtered_rec = {}\n",
    "for i in tqdm.tqdm(range(15000)):\n",
    "    RF_filtered_rec[i] = rec_track[i][pre_track[i].nlargest(keep_).index.tolist()].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tnum_val = 0\n",
    "suc_rec = 0\n",
    "num_p = 0\n",
    "for i in active_user:\n",
    "    num_val = len(v_ft[index_to_user_dict[i]].keys())\n",
    "    Tnum_val = Tnum_val+num_val\n",
    "    rec_set = RF_filtered_rec[i]\n",
    "    num_p = num_p+len(rec_set)\n",
    "    for j in v_ft[index_to_user_dict[i]].keys():\n",
    "        if j in rec_set:\n",
    "            suc_rec+=1\n",
    "            \n",
    "recall_nn = suc_rec/Tnum_val\n",
    "precision_nn = suc_rec/num_p\n",
    "print('RF with k-means filtered recall is',recall_nn)\n",
    "print('RF with k-means filtered precision is',precision_nn)\n",
    "f1 = 2*(recall_nn*precision_nn)/(precision_nn+recall_nn)\n",
    "print('F1 is',f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
